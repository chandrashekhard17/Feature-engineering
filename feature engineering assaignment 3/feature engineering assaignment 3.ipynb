{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b1b090d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Q1. **What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.**\n",
    "\n",
    "**Min-Max scaling** is a normalization technique that transforms data to a specified range, typically [0,1], by scaling each feature based on the minimum and maximum values of the feature. This helps ensure that all features contribute equally to the model, particularly when features have different units or magnitudes.\n",
    "\n",
    "**Formula:**\n",
    "\\[\n",
    "X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\n",
    "\\]\n",
    "Where \\(X_{\\text{min}}\\) and \\(X_{\\text{max}}\\) are the minimum and maximum values of the feature.\n",
    "\n",
    "**Example:**\n",
    "Suppose you have the following data: `[10, 20, 30, 40, 50]`. To apply Min-Max scaling to a range of [0,1]:\n",
    "\n",
    "- \\(X_{\\text{min}} = 10\\), \\(X_{\\text{max}} = 50\\)\n",
    "- For \\(X = 10\\): \\(X_{\\text{scaled}} = \\frac{10 - 10}{50 - 10} = 0\\)\n",
    "- For \\(X = 50\\): \\(X_{\\text{scaled}} = \\frac{50 - 10}{50 - 10} = 1\\)\n",
    "\n",
    "Resulting scaled data: `[0, 0.25, 0.5, 0.75, 1]`.\n",
    "\n",
    "---\n",
    "\n",
    "### Q2. **What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.**\n",
    "\n",
    "The **Unit Vector** technique scales each feature vector such that the Euclidean (L2) norm of the vector is 1. This means the data is transformed to lie on a unit sphere, making it useful for models sensitive to the direction rather than the magnitude of the data.\n",
    "\n",
    "**Formula:**\n",
    "\\[\n",
    "X_{\\text{unit}} = \\frac{X}{\\|X\\|}\n",
    "\\]\n",
    "Where \\(\\|X\\|\\) is the L2 norm of the vector.\n",
    "\n",
    "**Difference from Min-Max Scaling:**\n",
    "- **Min-Max scaling** normalizes data to a specific range (e.g., [0, 1]), whereas **Unit Vector scaling** ensures that the magnitude (length) of the vector is 1.\n",
    "- Unit Vector scaling is focused on the direction of the data, whereas Min-Max focuses on range.\n",
    "\n",
    "**Example:**\n",
    "For a vector \\([3, 4]\\):\n",
    "- L2 norm: \\(\\|X\\| = \\sqrt{3^2 + 4^2} = 5\\)\n",
    "- Unit Vector scaling: \\([3/5, 4/5] = [0.6, 0.8]\\)\n",
    "\n",
    "---\n",
    "\n",
    "### Q3. **What is PCA (Principal Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.**\n",
    "\n",
    "**Principal Component Analysis (PCA)** is a dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space by projecting the data onto a set of orthogonal (uncorrelated) axes called principal components. These components capture the maximum variance in the data, with the first principal component capturing the most variance, and each subsequent component capturing the next highest variance.\n",
    "\n",
    "**How it works:**\n",
    "1. Standardize the data.\n",
    "2. Compute the covariance matrix.\n",
    "3. Find the eigenvectors (principal components) and eigenvalues (variance captured by each component).\n",
    "4. Project the data onto the principal components.\n",
    "\n",
    "**Example:**\n",
    "Consider a dataset with two correlated features, \\(X_1\\) and \\(X_2\\). After applying PCA, you might find that the first principal component explains most of the variance in the data. By projecting onto this component, you reduce the dimensionality to one, capturing most of the information.\n",
    "\n",
    "---\n",
    "\n",
    "### Q4. **What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.**\n",
    "\n",
    "**PCA as Feature Extraction**: PCA transforms the original feature space into a set of new features (principal components), which are linear combinations of the original features. These new features are uncorrelated and ranked by the amount of variance they explain in the data.\n",
    "\n",
    "**Relationship**: Feature extraction using PCA is about finding the most informative combinations of features (principal components) that explain the variability in the data while reducing redundancy and noise.\n",
    "\n",
    "**Example:**\n",
    "In a dataset with 10 correlated features, applying PCA may show that 3 principal components explain 95% of the variance. These 3 components replace the original 10 features, making the data simpler and more interpretable.\n",
    "\n",
    "---\n",
    "\n",
    "### Q5. **You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.**\n",
    "\n",
    "To preprocess the data for the recommendation system using **Min-Max scaling**:\n",
    "1. Identify the features: price, rating, and delivery time. These features may have different units and ranges.\n",
    "2. Apply Min-Max scaling to each feature independently to bring them into the same range (e.g., [0,1]):\n",
    "   - **Price**: Normalize the range of prices from minimum to maximum.\n",
    "   - **Rating**: Normalize the range of ratings (e.g., from 1 to 5).\n",
    "   - **Delivery time**: Normalize delivery time (e.g., from 10 minutes to 60 minutes).\n",
    "3. This ensures that no single feature dominates the model, improving the performance of algorithms sensitive to feature scales (e.g., k-NN, SVM).\n",
    "\n",
    "---\n",
    "\n",
    "### Q6. **You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.**\n",
    "\n",
    "To use PCA for dimensionality reduction in predicting stock prices:\n",
    "1. **Standardize the features**: Since PCA is affected by the scale of the data, standardize the financial data and market trends to have mean 0 and variance 1.\n",
    "2. **Compute the covariance matrix**: This shows the relationships between features.\n",
    "3. **Find principal components**: Determine the eigenvectors (principal components) and eigenvalues (variance explained).\n",
    "4. **Select the top components**: Choose a number of principal components that explain a significant portion of the variance (e.g., 95% of the variance).\n",
    "5. **Transform the data**: Project the original dataset onto the selected principal components to create a lower-dimensional dataset that retains most of the information.\n",
    "\n",
    "---\n",
    "\n",
    "### Q7. **For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.**\n",
    "\n",
    "**Min-Max Scaling Formula** to transform to [-1, 1]:\n",
    "\\[\n",
    "X_{\\text{scaled}} = 2 \\times \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} - 1\n",
    "\\]\n",
    "Where \\(X_{\\text{min}} = 1\\) and \\(X_{\\text{max}} = 20\\).\n",
    "\n",
    "For each value:\n",
    "- \\(X = 1\\): \\(X_{\\text{scaled}} = 2 \\times \\frac{1 - 1}{20 - 1} - 1 = -1\\)\n",
    "- \\(X = 5\\): \\(X_{\\text{scaled}} = 2 \\times \\frac{5 - 1}{20 - 1} - 1 = -0.789\\)\n",
    "- \\(X = 10\\): \\(X_{\\text{scaled}} = 2 \\times \\frac{10 - 1}{20 - 1} - 1 = -0.474\\)\n",
    "- \\(X = 15\\): \\(X_{\\text{scaled}} = 2 \\times \\frac{15 - 1}{20 - 1} - 1 = -0.158\\)\n",
    "- \\(X = 20\\): \\(X_{\\text{scaled}} = 2 \\times \\frac{20 - 1}{20 - 1} - 1 = 1\\)\n",
    "\n",
    "Result: `[-1, -0.789, -0.474, -0.158, 1]`.\n",
    "\n",
    "---\n",
    "\n",
    "### Q8. **For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?**\n",
    "\n",
    "**Steps to perform Feature Extraction using PCA:**\n",
    "1. **Standardize the features**: Standardize the dataset (excluding gender, as it is categorical) to have a mean of 0 and standard deviation of 1.\n",
    "2. **Compute principal components**: Apply PCA to compute the principal components based on the covariance matrix.\n",
    "3. **Choose the number of components**: Use the explained variance ratio to decide how many principal components to retain. For example, if the first 2 or 3 components explain 95% of the variance, retain those components.\n",
    "\n",
    "**Why retain 2-3 components?**: In most cases, a few principal components capture most of the variability, simplifying the model while maintaining predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252a8af0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
